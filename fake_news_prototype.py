# -*- coding: utf-8 -*-
"""Fake_News_Prototype.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqDmqJ6i0nAVbqagkuwus6Qo5Sy_BrCX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
import random

from nltk.stem.porter import PorterStemmer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

fake = pd.read_csv('Fake.csv')
fake

fake["subject"].value_counts()

real = pd.read_csv('True.csv')
real

unknown_publishers = []
for index, row in enumerate(real.text.values):
  try:
    record = row.split('-', maxsplit=1)
    record[1]
    assert(len(record[0])<120)
  except:
    unknown_publishers.append(index)

real.iloc[unknown_publishers].text

publisher = []
tmp_text = []

for index, row in enumerate(real.text.values):
  if index in unknown_publishers:
    tmp_text.append(row)
    publisher.append('Unknown')
  else:
    record = row.split('-', maxsplit=1)
    publisher.append(record[0].strip())
    tmp_text.append(record[1].strip())

real['publisher'] = publisher
real['text'] = tmp_text

real.head()

empty_fake_index = [index for index, text in enumerate(fake.text.tolist()) if str(text).strip() == ""]

fake.iloc[empty_fake_index]

real['text'] = real['title'] + " " + real['text']
fake['text'] = fake['title'] + " " + fake['text']

real['class'] = 1
fake['class'] = 0

real = real[['text', 'class']]
fake = fake[['text', 'class']]

data = real.append(fake, ignore_index=True)
data

ps = PorterStemmer()

def preprocess_text(_text):
  _text = _text.lower()
  _text = re.sub(r'\$[^\s]+', 'dollar', _text)
  _text = re.sub(r'[^a-z0-9\s]', '', _text)
  _text = re.sub(r'[0-9]+', 'number', _text)
  _text = _text.split(" ")
  _text = list(map(lambda x: ps.stem(x), _text))
  _text = list(map(lambda x: x.strip(), _text))
  if '' in _text:
    _text.remove('')

  return _text

data.text = data['text'].apply(preprocess_text)
data

import gensim

y = data['class'].values
x = [d for d in data['text'].tolist()]
DIM = 100
w2v_model = gensim.models.Word2Vec(sentences=x, size=DIM, window=10, min_count=1)

w2v_model.wv.most_similar('book')

tokenizer = Tokenizer()
tokenizer.fit_on_texts(x)
x = tokenizer.texts_to_sequences(x)

nos = np.array([len(i) for i in x])
maxlen = 1000
x = pad_sequences(x, maxlen=maxlen)
vocab_size = len(tokenizer.word_index) + 1
vocab = tokenizer.word_index

def get_weight_matrix(model):

  weight_matrix = np.zeros((vocab_size, DIM))

  for word, i in vocab.items():
    weight_matrix[i] = model.wv[word]
  
  return weight_matrix

embedding_vectors = get_weight_matrix(w2v_model)

model = Sequential()
model.add(Embedding(vocab_size, output_dim=DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))
model.add(LSTM(units=256))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

model.summary()

x_train, x_test, y_train, y_test = train_test_split(x, y)
model.fit(x_train, y_train, validation_split=0.3, epochs=6)

y_pred = (model.predict(x_test) >= 0.5).astype(int)
y_pred

accuracy_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""## Main Project"""

news = input("Enter some news: ")
some_text = [news]
some_text = tokenizer.texts_to_sequences(some_text)
some_text = pad_sequences(some_text, maxlen=maxlen)

print("News are " + ("True" if (model.predict(some_text) >= 0.5).astype(int) == 1 else "Fake"))